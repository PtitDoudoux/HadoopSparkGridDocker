---
version: '3'
 
services:
  # Apache Hadoop configuration
  namenode:
    image: bde2020/hadoop-namenode:1.2.1-hadoop2.8-java8
    restart: on-failure
    container_name: hadoop-namenode
    volumes:
      - hadoop-namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=ws_5ibd_al_td
    env_file:
      - ./hadoop-hive.env
    ports:
      - "50070:50070"
  datanode1:
    image: bde2020/hadoop-datanode:1.2.1-hadoop2.8-java8
    restart: on-failure
    container_name: hadoop-datanode-1
    depends_on: 
      - namenode
    volumes:
      - hadoop-datanode-1:/hadoop/dfs/data
    env_file:
      - ./hadoop-hive.env
    ports:
      - "50075:50075"
  datanode2:
    image: bde2020/hadoop-datanode:1.2.1-hadoop2.8-java8
    restart: on-failure
    container_name: hadoop-datanode-2
    depends_on: 
      - namenode
    volumes:
      - hadoop-datanode-2:/hadoop/dfs/data
    env_file:
      - ./hadoop-hive.env
    ports:
      - "50080:50075"
  # Apache Hive conf
  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    restart: on-failure
    container_name: hive-server
    env_file:
      - ./hadoop-hive.env
    environment:
      - "HIVE_CORE_CONF_javax_jdo_option_ConnectionURL=jdbc:postgresql://hive-metastore/metastore"
    ports:
      - "10000:10000"
    depends_on:
      - hive-metastore-postgresql
      - hive-metastore
  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    restart: on-failure
    container_name: hive-metastore
    depends_on:
      - hive-metastore-postgresql
    env_file:
      - ./hadoop-hive.env
    command: /opt/hive/bin/hive --service metastore
  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.3.0
    container_name: hive-metastore-postgresql
    depends_on:
      - namenode
  # Apache Spark configuration
  spark-master:
    image: bde2020/spark-master:2.2.0-hadoop2.8-hive-java8
    restart: on-failure
    container_name: spark-master
    ports:
      - "8880:8080"
      - "7077:7077"
    env_file:
      - ./hadoop-hive.env
  spark-worker-1:
    image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8
    restart: on-failure
    container_name: spark-worker-1
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - "8081:8081"
    env_file:
      - ./hadoop-hive.env
  spark-worker-2:
    image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8
    restart: on-failure
    container_name: spark-worker-2
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - "8082:8081"
    env_file:
      - ./hadoop-hive.env
  # Apache Zeppelin configuration
  zeppelin:
    build:
      context: .
      dockerfile: Dockerfile-Zeppelin
    restart: on-failure
    container_name: zeppelin
    ports:
      - "8080:8080"
    volumes:
      - zeppelin-notebook:/opt/zeppelin/notebook
    environment:
      CORE_CONF_fs_defaultFS: "hdfs://namenode:8020"
      SPARK_MASTER: "spark://spark-master:7077"
      MASTER: "spark://spark-master:7077"
    depends_on:
      - spark-master
      - namenode
  # Cloudera Hue configuration
  hue:
    image: bde2020/hdfs-filebrowser:3.11
    restart: on-failure
    container_name: hue
    ports:
      - "8088:8088"
    environment:
      - NAMENODE_HOST=namenode
  # Virtuoso configuration
  virtuoso:
    image: tenforce/virtuoso:1.3.2-virtuoso7.2.5.1
    restart: on-failure
    container_name: sparql_virtuoso
    environment:
      SPARQL_UPDATE: "true"
      DBA_PASSWORD: "dba"
      # DEFAULT_GRAPH: "https://query.wikidata.org"
    volumes:
      - virtuoso:/data
    ports:
      - "1111:1111"
      - "8890:8890"

volumes:
  hadoop-namenode:
  hadoop-datanode-1:
  hadoop-datanode-2:
  zeppelin-notebook:
  virtuoso:
...
